{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn = dimension of input\\nm = dimension of output\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n = dimension of input\n",
    "m = dimension of output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward_linear(input, weight, bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input [1 x n]      The input vector to the layer\n",
    "    weight [m x n]     The weight matrix of the layer\n",
    "    bias [m]           The bias vector of the layer\n",
    "    \n",
    "    Returns:\n",
    "    output [1 x m]     The input to the next layer = logit put through the non-linear activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate logit [1 x m], where logit = input.mm(weight.t()) + bias       \n",
    "    logit = input.mm(weight.t())\n",
    "    logit += bias.unsqueeze(0).expand_as(logit)\n",
    "    \n",
    "    # Non-linear activation function\n",
    "    output = logit\n",
    "    \n",
    "    return [output, logit]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def esgd_linear(grad_output, weight, input):\n",
    "    grad_input = grad_output.mm(weight)          # [1 x n]\n",
    "    grad_weight = grad_output.t().mm(input)      # [m x n]\n",
    "    grad_bias = grad_output.sum(0).squeeze(0)    # [m]\n",
    "\n",
    "    return [grad_input, grad_weight, grad_bias]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def a_linear(s,d,c):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    s [1 x m]      Sign of back-propagated gradient\n",
    "    d [1 x m]      Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "    c [1 x m]      Logit contracted by ridge-regularization\n",
    "    \n",
    "    Return\n",
    "    alpha [1 x m]  Solution of ISGD update for each output\n",
    "    \"\"\"\n",
    "    alpha = - s * d  # Note that this is element-wise multiplication\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Old version which seems unstable wrt the learning rate\n",
    "def isgd_linear_old(grad_output, weight, input, lr, mu):\n",
    "    # ISGD constants\n",
    "    s = torch.sign(grad_output)     # [1 x m]\n",
    "    z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0)) # [1]\n",
    "    d = z_norm * math.sqrt(lr/(1.0+lr*mu)) * torch.sqrt(torch.abs(grad_output)) # [1 x m]\n",
    "    c = logit / (1.0+lr*mu) #  [1 x m]\n",
    "\n",
    "    # Calculate alpha\n",
    "    alpha = alpha_linear(s,d,c) # [1 x m]\n",
    "\n",
    "    # Calculate new weight, bias, and the implied gradients\n",
    "    new_weight = weight / (1.0 + lr * mu) + (alpha * d).t().mm(input) / z_norm **2  # [m x n]\n",
    "    grad_weight = (weight - new_weight) / lr  #  [m x n]\n",
    "\n",
    "    new_bias = bias / (1.0 + lr * mu) + (alpha * d).squeeze() / z_norm **2  # [m]\n",
    "    grad_bias = (bias - new_bias) / lr  # [m]\n",
    "    \n",
    "    grad_input = grad_output.mm(weight)\n",
    "\n",
    "    # Return the results\n",
    "    return [grad_input, grad_weight, grad_bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_linear(grad_output, weight, input, logit):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lr = 0.001\n",
    "    mu = 0.0\n",
    "\n",
    "    # ISGD constants\n",
    "    s = torch.sign(grad_output)     # [1 x m]\n",
    "    z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0)) # [1]\n",
    "    d = z_norm / math.sqrt(1.0+lr*mu) * torch.sqrt(torch.abs(grad_output)) # [1 x m]\n",
    "    c = logit / (1.0+lr*mu) #  [1 x m]\n",
    "\n",
    "    # Calculate alpha\n",
    "    a = a_linear(s,d,c) # [1 x m]\n",
    "\n",
    "    # Calculate new weight, bias, and the implied gradients\n",
    "    grad_weight = weight * mu / (1.0 + lr * mu) - (a * d).t().mm(input) / z_norm **2  #  [m x n]\n",
    "\n",
    "    grad_bias = bias * mu / (1.0 + lr * mu) - (a * d).squeeze() / z_norm **2  #  [m x n]\n",
    "    \n",
    "    grad_input = grad_output.mm(weight)\n",
    "\n",
    "    # Return the results\n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the differences between ESGD and ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random data\n",
    "grad_output = torch.randn(1, 3)     # [1 x m]\n",
    "input = torch.randn(1, 2)           # [1 x n]\n",
    "weight = torch.randn(3, 2)          # [m x n]\n",
    "bias = torch.randn(3,)              # [m]\n",
    "\n",
    "# Check that forward propagation makes sense\n",
    "output, logit = forward_linear(input, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 0  0\n",
      "[torch.FloatTensor of size 1x2]\n",
      ", \n",
      "1.00000e-07 *\n",
      "  0.5960 -0.5960\n",
      "  0.0000 -1.1921\n",
      "  0.5960  0.0000\n",
      "[torch.FloatTensor of size 3x2]\n",
      ", \n",
      "1.00000e-07 *\n",
      "  0.5960\n",
      "  1.1921\n",
      "  0.5960\n",
      "[torch.FloatTensor of size 3]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "isgd_grads = isgd_linear(grad_output, weight, input, logit)\n",
    "esgd_grads = esgd_linear(grad_output, weight, input)\n",
    "\n",
    "print([(x-y) for x,y in zip(isgd_grads, esgd_grads)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
