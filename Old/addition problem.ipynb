{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"Addition dataset as introduced in the original LSTM paper.\n",
    "    This implementation is from p.11 of 'On the difficulty of training recurrent neural networks' \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_length, len_sequence):\n",
    "        self.dataset_length = dataset_length  # This is what is returned by len(), see def __len__(self) below\n",
    "        self.t = len_sequence  # Length of sequence\n",
    "        # Check that sequence length is at least 10\n",
    "        # If not, there is no randomness in the position of the first number to be added\n",
    "        assert (self.t > 10), 'Sequence length must be at least 10'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        # The dummy index is required for the dataloader to work,\n",
    "        # but since we are sampling data randomly it has no effect\n",
    "\n",
    "        # Sample the length of the sequence and positions of numbers to add\n",
    "        t_dash = np.random.randint(self.t, int(self.t * 11.0 / 10.0))  # Length of the sequence\n",
    "        t_1 = np.random.randint(0, int(t_dash / 10.0))  # Indicator of position of first number to add\n",
    "        t_2 = np.random.randint(int(t_dash / 10.0), int(t_dash / 2.0))  # Indicator of position of second number to add\n",
    "\n",
    "        # We generate random numbers uniformly sampled from [0,1]\n",
    "        # as depicted in Figure 2 of\n",
    "        # \"Learning Recurrent Neural Networks with Hessian-Free Optimization\"\n",
    "        # Details of how to sample the numbers was not given in\n",
    "        # \"On the difficulty of training recurrent neural networks\"\n",
    "        sequence = torch.zeros((2, t_dash))  # Initialize empty sequence\n",
    "        sequence[0, :] = torch.rand((1, t_dash))  # Make first row random numbers\n",
    "\n",
    "        # Set second row to indicate which numbers to add\n",
    "        sequence[1, t_1] = 1.0\n",
    "        sequence[1, t_2] = 1.0\n",
    "\n",
    "        # Calculate target\n",
    "        target = torch.Tensor([sequence[0, t_1] + sequence[0, t_2]])\n",
    "\n",
    "        # Collect sequence and target into a sample\n",
    "        sample = (sequence, target)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addition_problem(train_length, test_length, sequence_length, num_workers=4):\n",
    "    \"\"\"\n",
    "    This is the addition problem\n",
    "\n",
    "\n",
    "    Args:\n",
    "        train_length:       Number of training examples for each epoch\n",
    "        test_length:        Number of test examples for each test\n",
    "        sequence_length:    Length of each sequence\n",
    "        num_workers:        Number of workers loading the data\n",
    "\n",
    "    Returns:\n",
    "        train_loader    Loads training data\n",
    "        test_loader     Loads test data\n",
    "\n",
    "    \"\"\"\n",
    "    # Batch size should be 1 to prevent sequences in the same batch having different lengths\n",
    "    batch_size = 1\n",
    "\n",
    "    train_loader = DataLoader(AdditionDataset(train_length, sequence_length),\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers)\n",
    "    test_loader = DataLoader(AdditionDataset(test_length, sequence_length),\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden))\n",
    "        hidden = self.i2h(combined)\n",
    "#         hidden = nn.functional.sigmoid(hidden)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Isgd_LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Isgd_LSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.out2output = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        out, hidden = self.lstm(input, hidden)\n",
    "        output = self.out2output(out)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        h0 = Variable(torch.zeros(1, 1, self.hidden_size)) # [1 x 1 x h]\n",
    "        c0 = Variable(torch.zeros(1, 1, self.hidden_size)) # [1 x 1 x h]\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  Variable containing:\n",
      "(0 ,.,.) = \n",
      "  0.1434\n",
      "[torch.FloatTensor of size 1x1x1]\n",
      "\n",
      "hidden:  (Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.0619 -0.0115 -0.1607 -0.0478 -0.0190\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      ", Variable containing:\n",
      "(0 ,.,.) = \n",
      " -0.1449 -0.0194 -0.2813 -0.0877 -0.0633\n",
      "[torch.FloatTensor of size 1x1x5]\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# See if can do one feed forward through the LSTM\n",
    "input = data[:,0].unsqueeze(0).unsqueeze(0) # [1 x 1 x 2]\n",
    "h0 = Variable(torch.zeros(1, 1, n_hidden)) # [1 x 1 x h]\n",
    "c0 = Variable(torch.zeros(1, 1, n_hidden)) # [1 x 1 x h]\n",
    "hidden = (h0, c0)\n",
    "# print('input: ', input.size())\n",
    "# print('hidden: ', hidden[0].size())\n",
    "\n",
    "# lstm = nn.LSTM(input_size, n_hidden)\n",
    "# out2output = nn.Linear(n_hidden, 1)\n",
    "\n",
    "# out, hidden = lstm(input, hidden)\n",
    "# output = out2output(out)\n",
    "\n",
    "output, hidden = isgd_lstm(input, hidden)\n",
    "\n",
    "print('output: ', output)\n",
    "print('hidden: ', hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = AdditionDataset(dataset_length=8, len_sequence=11)\n",
    "train_loader, test_loader = addition_problem(train_length=8, test_length=9, sequence_length=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.7184  0.2618  0.5362  0.8539  0.5778  0.3738  0.2416  0.4440  0.0909  0.9868\n",
      " 1.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.9027\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n",
      "Variable containing:\n",
      " 1.2963\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a single sample from the dataset\n",
    "for i_batch, (data, target) in enumerate(test_loader):\n",
    "    data = Variable(data[0,:,:]) # [1 x 2 x t]\n",
    "    target = Variable(target)\n",
    "    break\n",
    "\n",
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set neural network parameters\n",
    "n_hidden = 5\n",
    "input_size = 2\n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize RNN and lstm\n",
    "rnn = RNN(2, n_hidden, 1)\n",
    "isgd_lstm = Isgd_LSTM(2, n_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  0.2859\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       " -0.1871\n",
       " -0.1375\n",
       "  0.0956\n",
       "  0.1869\n",
       " -0.8787\n",
       " [torch.FloatTensor of size 5])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if can do one feed forward through the RNN\n",
    "input = data[:,0] # [2]\n",
    "hidden = Variable(torch.zeros(n_hidden)) # [h]\n",
    "\n",
    "combined = torch.cat((input, hidden)) # [h+2]\n",
    "\n",
    "i2h = nn.Linear(input_size + n_hidden, n_hidden)\n",
    "i2o = nn.Linear(input_size + n_hidden, output_size)\n",
    "\n",
    "hidden_new = i2h(combined)\n",
    "rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run one iteration of the rnn\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "rnn.train()\n",
    "for epoch in range(10):\n",
    "    for i_batch, (data, target) in enumerate(test_loader):\n",
    "        \n",
    "        data = Variable(data) # [1 x 2 x t]\n",
    "        target = Variable(target) # [1]\n",
    "\n",
    "        # Get rid of zeroth dimension, since the minibatch is of size 1\n",
    "        data = data[0,:,:] # [2 x t]\n",
    "        \n",
    "        hidden = rnn.initHidden() # [h]\n",
    "        hidden = hidden # [h]\n",
    "        rnn.zero_grad()\n",
    "\n",
    "        for i in range(data.size()[1]):\n",
    "            input = data[:,i] # [2]\n",
    "            output, hidden = rnn(input, hidden)\n",
    "\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        if i_batch == 0:\n",
    "            print('loss: ', float(loss))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.5140122771263123\n",
      "loss:  0.49200868606567383\n",
      "loss:  0.22727467119693756\n",
      "loss:  1.3614734411239624\n",
      "loss:  0.5532548427581787\n",
      "loss:  0.15613891184329987\n",
      "loss:  0.2769795358181\n",
      "loss:  2.326101303100586\n",
      "loss:  0.011181870475411415\n",
      "loss:  0.5309451818466187\n"
     ]
    }
   ],
   "source": [
    "# Run one iteration of the lstm\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.SGD(isgd_lstm.parameters(), lr=learning_rate)\n",
    "isgd_lstm.train()\n",
    "for epoch in range(10):\n",
    "    for i_batch, (data, target) in enumerate(test_loader):\n",
    "        data = Variable(data) # [1 x 2 x t]\n",
    "        target = Variable(target) # [1]\n",
    "        \n",
    "        \n",
    "        hidden = isgd_lstm.initHidden() # [h]\n",
    "        lstm.zero_grad()\n",
    "\n",
    "        for i in range(data.size()[1]):\n",
    "            input = data[:,:,i].unsqueeze(0) # [1 x 1 x 2]\n",
    "            output, hidden = isgd_lstm(input, hidden)\n",
    "\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        if i_batch == 0:\n",
    "            print('loss: ', float(loss))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the rnn\n",
    "rnn.eval()\n",
    "data, target = train_data[0]\n",
    "data = Variable(data) # [2 x t]\n",
    "target = Variable(target) # [1]\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "for i in range(data.size()[1]):\n",
    "    input = data[:,i]\n",
    "    output, hidden = rnn(input, hidden)\n",
    "    \n",
    "print('data: ', data)\n",
    "print('target: ', float(target))\n",
    "print('output: ', float(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
