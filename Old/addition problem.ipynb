{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"Addition dataset as introduced in the original LSTM paper.\n",
    "    This implementation is from p.11 of 'On the difficulty of training recurrent neural networks' \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_length, len_sequence):\n",
    "        self.dataset_length = dataset_length  # This is what is returned by len(), see def __len__(self) below\n",
    "        self.t = len_sequence  # Length of sequence\n",
    "        # Check that sequence length is at least 10\n",
    "        # If not, there is no randomness in the position of the first number to be added\n",
    "        assert (self.t > 10), 'Sequence length must be at least 10'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        # The dummy index is required for the dataloader to work,\n",
    "        # but since we are sampling data randomly it has no effect\n",
    "\n",
    "        # Sample the length of the sequence and positions of numbers to add\n",
    "        t_dash = np.random.randint(self.t, int(self.t * 11.0 / 10.0))  # Length of the sequence\n",
    "        t_1 = np.random.randint(0, int(t_dash / 10.0))  # Indicator of position of first number to add\n",
    "        t_2 = np.random.randint(int(t_dash / 10.0), int(t_dash / 2.0))  # Indicator of position of second number to add\n",
    "\n",
    "        # We generate random numbers uniformly sampled from [0,1]\n",
    "        # as depicted in Figure 2 of\n",
    "        # \"Learning Recurrent Neural Networks with Hessian-Free Optimization\"\n",
    "        # Details of how to sample the numbers was not given in\n",
    "        # \"On the difficulty of training recurrent neural networks\"\n",
    "        sequence = torch.zeros((2, t_dash))  # Initialize empty sequence\n",
    "        sequence[0, :] = torch.rand((1, t_dash))  # Make first row random numbers\n",
    "\n",
    "        # Set second row to indicate which numbers to add\n",
    "        sequence[1, t_1] = 1.0\n",
    "        sequence[1, t_2] = 1.0\n",
    "\n",
    "        # Calculate target\n",
    "        target = torch.Tensor([sequence[0, t_1] + sequence[0, t_2]])\n",
    "\n",
    "        # Collect sequence and target into a sample\n",
    "        sample = (sequence, target)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addition_problem(train_length, test_length, sequence_length, num_workers=4):\n",
    "    \"\"\"\n",
    "    This is the addition problem\n",
    "\n",
    "\n",
    "    Args:\n",
    "        train_length:       Number of training examples for each epoch\n",
    "        test_length:        Number of test examples for each test\n",
    "        sequence_length:    Length of each sequence\n",
    "        num_workers:        Number of workers loading the data\n",
    "\n",
    "    Returns:\n",
    "        train_loader    Loads training data\n",
    "        test_loader     Loads test data\n",
    "\n",
    "    \"\"\"\n",
    "    # Batch size should be 1 to prevent sequences in the same batch having different lengths\n",
    "    batch_size = 1\n",
    "\n",
    "    train_loader = DataLoader(AdditionDataset(train_length, sequence_length),\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers)\n",
    "    test_loader = DataLoader(AdditionDataset(test_length, sequence_length),\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden))\n",
    "        hidden = self.i2h(combined)\n",
    "#         hidden = nn.functional.sigmoid(hidden)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_data = AdditionDataset(dataset_length=8, len_sequence=11)\n",
    "train_loader, test_loader = addition_problem(train_length=8, test_length=9, sequence_length=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.3710  0.8183  0.0180  0.0268  0.4146  0.8385  0.3395  0.8989  0.0455  0.4677\n",
      " 1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.4287\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n",
      "Variable containing:\n",
      " 0.3890\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a single sample from the dataset\n",
    "for i_batch, (data, target) in enumerate(test_loader):\n",
    "    data = Variable(data[0,:,:]) # [b x 2 x t]\n",
    "    target = Variable(target)\n",
    "    break\n",
    "\n",
    "print(data)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize RNN with its parameters\n",
    "n_hidden = 5\n",
    "input_size = 2\n",
    "output_size = 1\n",
    "rnn = RNN(2, n_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " 1.00000e-02 *\n",
       "  -3.0359\n",
       " [torch.FloatTensor of size 1], Variable containing:\n",
       " -0.0033\n",
       "  0.0803\n",
       " -0.3669\n",
       "  0.2482\n",
       "  0.2287\n",
       " [torch.FloatTensor of size 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See if can do one feed forward through the RNN\n",
    "input = data[:,0] # [2]\n",
    "hidden = Variable(torch.zeros(n_hidden)) # [h]\n",
    "\n",
    "combined = torch.cat((input, hidden)) # [h+2]\n",
    "\n",
    "i2h = nn.Linear(input_size + n_hidden, n_hidden)\n",
    "i2o = nn.Linear(input_size + n_hidden, output_size)\n",
    "\n",
    "hidden_new = i2h(combined)\n",
    "rnn(input, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  2.2251141729157098e-07\n",
      "loss:  0.2141166776418686\n",
      "loss:  0.12519675493240356\n",
      "loss:  0.12402684986591339\n",
      "loss:  0.38254186511039734\n",
      "loss:  0.9527448415756226\n",
      "loss:  0.1618058681488037\n",
      "loss:  0.04821506142616272\n",
      "loss:  0.5575642585754395\n",
      "loss:  0.4382413923740387\n"
     ]
    }
   ],
   "source": [
    "# Run one iteration of the rnn\n",
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)\n",
    "rnn.train()\n",
    "for epoch in range(10):\n",
    "    for i_batch, (data, target) in enumerate(test_loader):\n",
    "        \n",
    "        data = Variable(data) # [1 x 2 x t]\n",
    "        target = Variable(target) # [1]\n",
    "\n",
    "        # Get rid of zeroth dimension, since the minibatch is of size 1\n",
    "        data = data[0,:,:] # [2 x t]\n",
    "        \n",
    "        hidden = rnn.initHidden() # [h]\n",
    "        hidden = hidden # [h]\n",
    "        rnn.zero_grad()\n",
    "\n",
    "        for i in range(data.size()[1]):\n",
    "            input = data[:,i] # [2]\n",
    "            output, hidden = rnn(input, hidden)\n",
    "\n",
    "        loss = nn.MSELoss()(output, target)\n",
    "        if i_batch == 0:\n",
    "            print('loss: ', float(loss))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  Variable containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      " 0.2205  0.3751  0.2210  0.4425  0.2733  0.4703  0.7826  0.0407  0.0830  0.5396\n",
      " 1.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 10 to 10 \n",
      " 0.2648\n",
      " 0.0000\n",
      "[torch.FloatTensor of size 2x11]\n",
      "\n",
      "target:  0.5955771207809448\n",
      "output:  0.8660391569137573\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the rnn\n",
    "rnn.eval()\n",
    "data, target = train_data[0]\n",
    "data = Variable(data) # [2 x t]\n",
    "target = Variable(target) # [1]\n",
    "hidden = rnn.initHidden()\n",
    "\n",
    "for i in range(data.size()[1]):\n",
    "    input = data[:,i]\n",
    "    output, hidden = rnn(input, hidden)\n",
    "    \n",
    "print('data: ', data)\n",
    "print('target: ', float(target))\n",
    "print('output: ', float(output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
