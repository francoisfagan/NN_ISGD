{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n =  2  # dimension of input\n",
    "m =  3  # dimension of output\n",
    "batch =  4  # batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_logit(input, weight, bias=None):\n",
    "    \"\"\"\n",
    "    Calculate logit = input.mm(weight.t()) + bias\n",
    "\n",
    "    Args:\n",
    "        input:  [1 x n]         Input vector\n",
    "        weight:  [m x n]        Weight matrix\n",
    "        bias:  [m]              Bias vector\n",
    "\n",
    "    Returns:\n",
    "        logit: [1 x n]          Logit = input.mm(weight.t()) + bias\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logit = input.mm(weight.t())\n",
    "    if bias is not None:\n",
    "        logit += bias.unsqueeze(0).expand_as(logit)\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def esgd_arctan(grad_output, input, weight, bias, output):\n",
    "\n",
    "    grad_output_scaled = grad_output / (1 + logit ** 2)  # [1 x m]\n",
    "    grad_input = grad_output_scaled.mm(weight)  # [1 x n]\n",
    "    grad_weight = grad_output_scaled.t().mm(input)  # [m x n]\n",
    "    grad_bias = grad_output_scaled.sum(0).squeeze(0)  # [m]\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old ISGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_backwards_variables(saved_tensors, logit, grad_output, lr, mu):\n",
    "    \"\"\"\n",
    "    Calculate the variables required for back-propagation\n",
    "    \n",
    "    Args:\n",
    "        saved_tensors:          Stores from forward-propagation the input, weight, bias, output\n",
    "        logit: [1 x n]          Stores from forward-propagation the logit\n",
    "        grad_output: [1 x m]    The gradient that has been back-propagated to this layer\n",
    "        lr: [1]                 Learning rate\n",
    "        mu: [1]                 Ridge-regularization constant\n",
    "\n",
    "    Returns:\n",
    "        input: [1 x n]          Input vector\n",
    "        weight: [m x n]         Weight matrix\n",
    "        bias: [m]               Bias vector\n",
    "        output [1 x m]          Input to the next layer = logit put through the non-linear activation function\n",
    "        logit: [1 x n]          Logit\n",
    "        s: [1 x m]              Sign of back-propagated gradient\n",
    "        z_norm: [1]             2-norm of (input, 1)\n",
    "        d: [1 x m]              Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "        c: [1 x m]              Logit contracted by ridge-regularization\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack saved values\n",
    "    input, weight, bias, output = saved_tensors\n",
    "\n",
    "    # ISGD constants\n",
    "    s = torch.sign(grad_output)\n",
    "    z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0))\n",
    "    d = z_norm / math.sqrt(1.0 + lr * mu) * torch.sqrt(torch.abs(grad_output))\n",
    "    c = logit / (1.0 + lr * mu)\n",
    "\n",
    "    return input, weight, bias, output, logit, s, z_norm, d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the weight matrix and bias vector\n",
    "\n",
    "    Args:\n",
    "        weight: [m x n]         Weight matrix\n",
    "        mu: [1]                 Ridge-regularization constant\n",
    "        lr: [1]                 Learning rate\n",
    "        a: [1 x m]              Solution of ISGD update\n",
    "        d: [1 x m]              Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "        input: [1 x n]          Input vector\n",
    "        z_norm: [1]             2-norm of (input, 1)\n",
    "        bias: [m]               Bias vector\n",
    "\n",
    "    Returns:\n",
    "        grad_weight: [m x n]    Gradient of the weight matrix\n",
    "        grad_bias: [m]          Gradient of the bias vector\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    grad_weight = weight * mu / (1.0 + lr * mu) - (a * d).t().mm(input) / z_norm ** 2\n",
    "    grad_bias = bias * mu / (1.0 + lr * mu) - (a * d).squeeze() / z_norm ** 2\n",
    "    return grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_root_closest_to_zero(coeff):\n",
    "    roots = np.roots(coeff)\n",
    "    real_roots = [root.real for root in roots if root.imag == 0]\n",
    "    root_closest_to_zero = reduce((lambda x, y: x if (abs(x) < abs(y)) else y), real_roots)\n",
    "    return root_closest_to_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isgd_arctan(grad_output, input, weight, bias, output, logit):\n",
    "    \n",
    "    lr, mu = 0.00000001, 0.0\n",
    "    input, weight, bias, output, logit, s, z_norm, d, c = calc_backwards_variables([input, weight, bias, output], logit,\n",
    "                                                                                       grad_output, lr, mu)\n",
    "\n",
    "    # Calculate a\n",
    "    coeff = np.array([((lr * d) ** 2).numpy()[0],\n",
    "                      (2 * lr * d * c).numpy()[0],\n",
    "                      (c ** 2 + 1).numpy()[0],\n",
    "                      (s * d).numpy()[0]])\n",
    "\n",
    "    root_closest_to_zero = np.apply_along_axis(real_root_closest_to_zero, 0, coeff)\n",
    "    a = torch.from_numpy(root_closest_to_zero).unsqueeze(1).t().type(torch.FloatTensor)\n",
    "\n",
    "    # Calculate grad_weight, grad_bias and return all gradients\n",
    "    grad_weight, grad_bias = calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias)\n",
    "\n",
    "    # Calculate input gradient\n",
    "    grad_output_scaled = grad_output / (1 + logit ** 2)  # [1 x m]\n",
    "    grad_input = grad_output_scaled.mm(weight)  # [1 x n]\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_arctan2(grad_output, input, weight, bias, output, logit):\n",
    "    \n",
    "    lr, mu = 0.0, 0.0\n",
    "    input, weight, bias, output, logit, s, z_norm, d, c = calc_backwards_variables([input, weight, bias, output], logit,\n",
    "                                                                                       grad_output, lr, mu)\n",
    "\n",
    "    # Calculate a\n",
    "    d_d = d.double()\n",
    "    s_m_d = (s * d).double()\n",
    "    c_d = c.double()\n",
    "\n",
    "    a = d_d * 0  # [b x m]\n",
    "    a_diff = 1  # Norm difference between previous and current a values\n",
    "    iter_count = 0  # Count of number of a iterations\n",
    "    while a_diff > 1e-15:\n",
    "        a_new = - s_m_d / (1.0 + (lr * d_d * a + c_d) ** 2)  # [b x m]\n",
    "        a_diff = torch.norm(a - a_new)\n",
    "        a = a_new  # [b x m]\n",
    "        iter_count += 1\n",
    "        if iter_count >= 50:\n",
    "            assert (iter_count < 50), 'Arctan update has failed to converge'\n",
    "\n",
    "    # Make a float so that can be operated on with other tensors\n",
    "    a = a.float()\n",
    "\n",
    "    # Calculate grad_weight, grad_bias and return all gradients\n",
    "    grad_weight, grad_bias = calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias)\n",
    "\n",
    "    # Calculate input gradient\n",
    "    grad_output_scaled = grad_output / (1 + logit ** 2)  # [b x m]\n",
    "    grad_input = grad_output_scaled.mm(weight)  # [b x n]\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New ISGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_new_arctan(input, weight, bias, output, logit, grad_output):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lr = 0.00000001\n",
    "    mu = 0.0\n",
    "\n",
    "    # ISGD constants\n",
    "    z_norm_squared = (torch.norm(input, p=2, dim=1) ** 2 + 1.0).double()  # [b]\n",
    "    c = (logit / (1.0 + lr * mu)).double()  # [b x m]\n",
    "\n",
    "    # Calculate u\n",
    "    # Calculate conditions for u\n",
    "\n",
    "#     b = torch.mul(z_norm_squared, grad_output.t()).t() * lr / (1 + lr * mu)\n",
    "#     v = cube_solver(b, c)\n",
    "#     u = torch.div(v.t(), lr * z_norm_squared).t()\n",
    "\n",
    "    b = torch.mul(z_norm_squared, grad_output.t()).t() * lr / (1 + lr * mu)\n",
    "    v = cube_solver(b, c)\n",
    "    u = torch.div(v.t(), lr * z_norm_squared).t()\n",
    "\n",
    "    u = u_arctan()\n",
    "\n",
    "    print('v_diff: ', v_diff(grad_output, c, lr, z_norm_squared, v))\n",
    "    print('u_diff: ', u_diff(grad_output, c, lr, z_norm_squared, u))\n",
    "\n",
    "    # Calculate input gradient\n",
    "    grad_output_scaled = grad_output / (1 + logit ** 2)  # [b x m]\n",
    "    grad_input = grad_output_scaled.mm(weight)  # [b x n]\n",
    "\n",
    "    # Calculate grad_weight, grad_bias\n",
    "    grad_weight = weight * mu / (1.0 + lr * mu) + u.t().mm(input)  # [m x n]\n",
    "    grad_bias = bias * mu / (1.0 + lr * mu) + u.t().sum(1)  # [m]\n",
    "    \n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def u_arctan(z_norm_squared, grad_output, lr, mu):\n",
    "    b = torch.mul(z_norm_squared, grad_output.t()).t() * lr / (1 + lr * mu)\n",
    "    v = cube_solver(b, c)\n",
    "    u = torch.div(v.t(), lr * z_norm_squared).t()\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cube_solver(b, c):\n",
    "    \"\"\" Solves for v in the equation:  v * (1 + (v - c)**2) = b\n",
    "    \"\"\"\n",
    "    b = b.double()\n",
    "    c = c.double()\n",
    "    \n",
    "    delta = 27 * (b ** 2) - 4 * b * (c ** 3) - 36 * b * c + 4 * (c ** 4) + 8 * (c ** 2) + 4  # [b x m]\n",
    "    gamma = 27 * b - 2 * (c ** 3) - 18 * c  # [b x m]\n",
    "    beta = (3 * ((3 * delta) ** (1/2)) + gamma)\n",
    "    cr_beta = torch.sign(beta) * (torch.abs(beta) ** (1/3))\n",
    "    v = cr_beta / (3 * (2 ** (1/3))) - (2 ** (1/3)) * (3 - c**2) / (3 * cr_beta) + 2 * c / 3\n",
    "    \n",
    "#     # Check that the solution is valid\n",
    "#     diff = x * (1 + (x - c)**2) - b\n",
    "#     print(diff)\n",
    "\n",
    "    return v.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the differences between ESGD, ISGD old and ISGD_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def u_diff(grad_output, c, lr, z_norm_squared, u):\n",
    "    return u - grad_output / (1.0 + (c - lr * torch.mul(z_norm_squared, u.t()).t()) ** 2) / (1.0 + lr * mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def v_diff(grad_output, c, lr, z_norm_squared, v):\n",
    "    b = torch.mul(z_norm_squared, grad_output.t()).t() * lr / (1 + lr * mu)\n",
    "    return v - b / (1.0 + (c - v) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-13 *\n",
      "  2.4799 -0.0142 -0.0006\n",
      " -0.0012 -0.0093  0.0022\n",
      " -0.0108 -0.0038 -0.0014\n",
      "  0.0047  0.0220 -0.0003\n",
      "[torch.DoubleTensor of size 4x3]\n",
      "\n",
      "v_diff:  \n",
      "1.00000e-14 *\n",
      "  5.4179 -0.0222  0.0000\n",
      "  0.0000  0.0444 -0.0888\n",
      " -0.0444 -0.0888  0.0000\n",
      "  0.0444  0.0222 -0.0444\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "u_diff:  \n",
      "1.00000e-06 *\n",
      "  2.4438 -0.0149  0.0000\n",
      " -0.1192  0.0149 -0.0298\n",
      " -0.0298  0.0000  0.0298\n",
      "  0.0149  0.0075  0.0000\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      "Difference between ESGD and ISGD new\n",
      "[\n",
      " 0  0\n",
      " 0  0\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.FloatTensor of size 4x2]\n",
      ", \n",
      "1.00000e-06 *\n",
      " -2.6226 -1.4305\n",
      "  0.0298  0.0298\n",
      " -0.0596 -0.0596\n",
      "[torch.FloatTensor of size 3x2]\n",
      ", \n",
      "1.00000e-06 *\n",
      "  2.2650\n",
      "  0.0000\n",
      " -0.0149\n",
      "[torch.FloatTensor of size 3]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Random data\n",
    "grad_output = torch.randn(batch, m)     # [b x m]\n",
    "input = torch.randn(batch, n)           # [b x n]\n",
    "weight = torch.randn(m, n)          # [m x n]\n",
    "bias = torch.randn(m,)              # [m]\n",
    "\n",
    "# Forward propagation\n",
    "# Calculate logit [1 x m], where logit = input.mm(weight.t()) + bias\n",
    "logit = calc_logit(input, weight, bias)\n",
    "\n",
    "# Non-linear activation function\n",
    "output = torch.atan(logit)  # [1 x m]\n",
    "\n",
    "# Calculate gradients\n",
    "esgd_grads = esgd_arctan(grad_output, input, weight, bias, output)\n",
    "isgd_grads = isgd_arctan2(grad_output, input, weight, bias, output, logit)\n",
    "isgd_new_grads = isgd_new_arctan(input, weight, bias, output, logit, grad_output)\n",
    "\n",
    "# print(esgd_grads)\n",
    "# print(isgd_new_grads)\n",
    "# Print difference\n",
    "# print('Difference between ESGD and ISGD old')\n",
    "# print([(x-y) for x,y in zip(isgd_grads, esgd_grads)])\n",
    "\n",
    "print('\\nDifference between ESGD and ISGD new')\n",
    "print([(x-y) for x,y in zip(isgd_new_grads, esgd_grads)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -208.8467   114.0256  2164.5085\n",
      "-3141.7490 -5193.1665 -4957.5005\n",
      " 5208.5259  7695.1826 -3666.8020\n",
      " 2619.1262 -1915.1833 -3137.2427\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.0001\n",
    "mu = 0.0\n",
    "\n",
    "# ISGD constants\n",
    "s = torch.sign(grad_output)  # [b x m]\n",
    "z_norm_squared = torch.norm(input, p=2, dim=1) ** 2 + 1.0  # [b]\n",
    "c = logit / (1.0 + lr * mu)  # [b x m]\n",
    "\n",
    "b = torch.mul(z_norm_squared, grad_output.t()).t() * lr / (1 + lr * mu)\n",
    "v = cube_solver(b, c)\n",
    "u = torch.div(grad_output.t(), lr * z_norm_squared).t()\n",
    "print(u)\n",
    "\n",
    "# print('delta: ', delta)\n",
    "# print('gamma: ', gamma)\n",
    "# print('beta: ', beta)\n",
    "# print('u: ', u)\n",
    "\n",
    "grad_weight = weight * mu / (1.0 + lr * mu) + u.t().mm(input)  # [m x n]\n",
    "grad_bias = bias * mu / (1.0 + lr * mu) + u.t().sum(1)  # [m]\n",
    "\n",
    "# u if it were created using esgd\n",
    "u_esgd = grad_output / (1 + c ** 2)\n",
    "\n",
    "# print('grad_weight: ', grad_weight)\n",
    "# print('grad_bias: ', grad_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-11 *\n",
       "  0.0000  0.0000  0.0000\n",
       "  0.0000  0.3638 -0.3638\n",
       "  0.0000  1.4552  0.3638\n",
       "  0.0000  0.0000 -0.0909\n",
       "[torch.FloatTensor of size 4x3]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_diff(grad_output, c, lr, z_norm_squared, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1.00000e-11 *\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  0.3638 -0.3638\n",
      "  0.0000  1.4552  0.3638\n",
      "  0.0000  0.0000 -0.0909\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      "1.00000e-08 *\n",
      "  0.0000  0.1863  0.0000\n",
      "  2.9802  0.0000 -2.9802\n",
      "  0.0000  5.9605  0.0000\n",
      "  0.0000 -2.9802 -0.7451\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      "-0.0437  0.0156  0.0824\n",
      "-0.3537 -0.3011 -0.3721\n",
      " 0.8572  0.6016 -0.2674\n",
      " 0.3294 -0.4565 -0.0494\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "b = torch.mul(z_norm_squared, grad_output.t()).t() * lr / (1 + lr * mu)\n",
    "print(v - b / (1.0 + (c - v) ** 2))\n",
    "u = torch.div(v.t(), lr * z_norm_squared).t()\n",
    "# print(u - torch.div((b / (1.0 + (c - v) ** 2) ).t(), lr * z_norm_squared).t())\n",
    "print(u - grad_output / (1.0 + (c - v) ** 2) / (1 + lr * mu) )\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explicit u_diff:  \n",
      "1.00000e-04 *\n",
      " -0.0490  0.0799  0.1079\n",
      " -0.8857  0.1895  0.0288\n",
      "  0.9292  0.1058  0.0031\n",
      " -0.0018 -0.0533 -2.1857\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "implicit u_diff:  \n",
      "1.00000e-08 *\n",
      "  0.0000  2.9802  0.0000\n",
      "  0.0000  0.0000  0.0000\n",
      "  0.0000  1.4901  0.0000\n",
      "  0.1863  0.0000  5.9605\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If accurate these should all be zero\n",
    "print('explicit u_diff: ', u_diff(grad_output, c, lr, z_norm_squared, u_esgd))\n",
    "print('implicit u_diff: ', u_diff(grad_output, c, lr, z_norm_squared, u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta:  \n",
      "   56.1127     8.9971    40.9222\n",
      "  110.5813     4.0082  2267.0413\n",
      "    6.0503   179.2418   415.8600\n",
      " 1525.8562    34.0047     4.9125\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "gamma:  \n",
      " -38.9227   13.4318   33.2091\n",
      " -54.7136    0.5707  303.7489\n",
      "   8.8526   70.1259  110.3616\n",
      "-237.0251  -30.2165   -5.9979\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "beta:  \n",
      " 8.4686e-04  2.9018e+01  6.6449e+01\n",
      "-7.2071e-02  1.0974e+01  5.5116e+02\n",
      " 2.1634e+01  1.3969e+02  2.1632e+02\n",
      "-3.4052e+01  8.4171e-02  5.5189e+00\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "cr_beta:  \n",
      " 0.0946  3.0729  4.0504\n",
      "-0.4162  2.2222  8.1989\n",
      " 2.7864  5.1887  6.0030\n",
      "-3.2413  0.4382  1.7672\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "v:  \n",
      "1.00000e-03 *\n",
      " -1.5748 -0.0314 -0.0369\n",
      " -0.2987  0.5080  0.0782\n",
      " -0.1945  0.0688  0.0129\n",
      " -0.0153  0.0650  0.4473\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "\n",
      "1.00000e-03 *\n",
      " -1.5748 -0.0314 -0.0369\n",
      " -0.2987  0.5080  0.0782\n",
      " -0.1945  0.0688  0.0129\n",
      " -0.0153  0.0650  0.4473\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta = 27 * (b ** 2) - 4 * b * (c ** 3) - 36 * b * c + 4 * (c ** 4) + 8 * (c ** 2) + 4  # [b x m]\n",
    "gamma = 27 * b - 2 * (c ** 3) - 18 * c  # [b x m]\n",
    "beta = (3 * ((3 * delta) ** (1/2)) + gamma)\n",
    "cr_beta = torch.sign(beta) * (torch.abs(beta) ** (1/3))\n",
    "v = cr_beta / (3 * (2 ** (1/3))) - (2 ** (1/3)) * (3 - c**2) / (3 * cr_beta) + 2 * c / 3\n",
    "\n",
    "print('delta: ', delta)\n",
    "print('gamma: ', gamma)\n",
    "print('beta: ', beta)\n",
    "print('cr_beta: ', cr_beta)\n",
    "print('v: ', v)\n",
    "print(cube_solver(b, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(u, c, lr, z_norm_squared, grad_output, mu):\n",
    "    return (u * (1.0 + c**2)\n",
    "                -2 * c * lr * torch.mul(z_norm_squared, (u**2).t()).t()\n",
    "                + (lr ** 2 * torch.mul(z_norm_squared** 2, (u** 3).t()).t())  \n",
    "      - grad_output / (1.0 + lr * mu))\n",
    "\n",
    "def f2(u, c, lr, z_norm_squared, grad_output, mu):\n",
    "    return ((1.0 + c**2)\n",
    "                -4 * c * lr * torch.mul(z_norm_squared, (u_esgd).t()).t()\n",
    "                + 3 *(lr ** 2 * torch.mul(z_norm_squared** 2, (u_esgd** 2).t()).t()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(u_diff(grad_output, c, lr, z_norm_squared, u_esgd))\n",
    "# print(u_esgd - grad_output / (1.0 + (c - lr * torch.mul(z_norm_squared, u_esgd.t()).t()) ** 2) / (1.0 + lr * mu))\n",
    "# print(u_esgd * (1.0 + (c - lr * torch.mul(z_norm_squared, u_esgd.t()).t()) ** 2) - grad_output / (1.0 + lr * mu))\n",
    "lr = 0.01\n",
    "# print(u_esgd * (1.0 + c**2\n",
    "#                 -2 * c * lr * torch.mul(z_norm_squared, u_esgd.t()).t()\n",
    "#                 + (lr * torch.mul(z_norm_squared, u_esgd.t()).t()) ** 2) \n",
    "#       - grad_output / (1.0 + lr * mu))\n",
    "f1 = f1(u_esgd, c, lr, z_norm_squared, grad_output, mu)\n",
    "f2 = f2(u_esgd, c, lr, z_norm_squared, grad_output, mu)\n",
    "\n",
    "u = u_esgd\n",
    "for i in range(10):\n",
    "#     print(u_diff(grad_output, c, lr, z_norm_squared, u))\n",
    "    u = u - f1 / f2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
