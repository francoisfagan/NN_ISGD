{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 10\n",
    "\n",
    "t_dash = np.random.randint(t, int(t*11.0/10.0))\n",
    "t_1 = np.random.randint(0, int(t_dash/10.0))\n",
    "t_2 = np.random.randint(int(t_dash/10.0), int(t_dash/2.0))\n",
    "\n",
    "print(t_dash)\n",
    "print(t_1)\n",
    "print(t_2)\n",
    "\n",
    "# We generate random numbers uniformly sampled from [0,1] \n",
    "# as depicted in Figure 2 of \"Learning Recurrent Neural Networks with Hessian-Free Optimization\"\n",
    "# Details of how to sample the numbers was not given in \"On the difficulty of training recurrent neural networks\"\n",
    "\n",
    "sequence = np.zeros((2,t_dash))\n",
    "sequence[0,:] = np.random.rand(t_dash)\n",
    "sequence[1,[t_1,t_2]] = 1.0\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"Addition dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, num_sequences, len_sequence):\n",
    "        self.num_sequences = num_sequences\n",
    "        self.t = len_sequence\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_sequences\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        t_dash = np.random.randint(self.t, int(t*11.0/10.0))\n",
    "        t_1 = np.random.randint(0, int(t_dash/10.0))\n",
    "        t_2 = np.random.randint(int(t_dash/10.0), int(t_dash/2.0))\n",
    "\n",
    "        sequence = np.zeros((2,t_dash))\n",
    "        sequence[0,:] = np.random.rand(t_dash)\n",
    "        sequence[1,[t_1,t_2]] = 1.0\n",
    "\n",
    "        return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addition_training_dataset = AdditionDataset(5,10)\n",
    "training_dataloader = DataLoader(addition_training_dataset, batch_size=4, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_batch, sample_batched in enumerate(training_dataloader):\n",
    "    print(i_batch)\n",
    "    print(sample_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AdditionDataset(Dataset):\n",
    "    \"\"\"Addition dataset as introduced in the original LSTM paper.\n",
    "    This implementation is from p.11 of 'On the difficulty of training recurrent neural networks' \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_length, len_sequence):\n",
    "        self.dataset_length = dataset_length  # This is what is returned by len(), see def __len__(self) below\n",
    "        self.t = len_sequence  # Length of sequence\n",
    "        # Check that sequence length is at least 10\n",
    "        # If not, there is no randomness in the position of the first number to be added\n",
    "        assert(self.t > 10), 'Sequence length must be at least 10'\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, dummy_index):\n",
    "        # The dummy index is required for the dataloader to work,\n",
    "        # but since we are sampling data randomly it has no effect\n",
    "\n",
    "        # Sample the length of the sequence and positions of numbers to add\n",
    "        t_dash = np.random.randint(self.t, int(self.t * 11.0 / 10.0) )  # Length of the sequence\n",
    "        t_1 = np.random.randint(0, int(t_dash / 10.0))  # Indicator of position of first number to add\n",
    "        t_2 = np.random.randint(int(t_dash / 10.0), int(t_dash / 2.0))  # Indicator of position of second number to add\n",
    "\n",
    "        # We generate random numbers uniformly sampled from [0,1]\n",
    "        # as depicted in Figure 2 of\n",
    "        # \"Learning Recurrent Neural Networks with Hessian-Free Optimization\"\n",
    "        # Details of how to sample the numbers was not given in\n",
    "        # \"On the difficulty of training recurrent neural networks\"\n",
    "        sequence = torch.zeros((2, t_dash))  # Initialize empty sequence\n",
    "        sequence[0, :] = torch.rand((1, t_dash))  # Make first row random numbers\n",
    "        \n",
    "        # Set second row to indicate which numbers to add\n",
    "        sequence[1, t_1] = 1.0 \n",
    "        sequence[1, t_2] = 1.0  \n",
    "\n",
    "        # Calculate target\n",
    "        target = sequence[0, t_1] + sequence[0, t_2]\n",
    "        \n",
    "        # Collect sequence and target into a sample\n",
    "        sample = {'sequence': sequence, 'target': target}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5541  0.5260  0.7278  0.2966  0.8558\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 2x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t_dash = 5\n",
    "sequence = torch.zeros((2, t_dash))\n",
    "sequence[0,:] = torch.rand((1, t_dash))\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addition_problem(train_dataset_length, test_data_length, len_sequence, batch_size=4, num_workers=4):\n",
    "    \"\"\"This is the addition problem\n",
    "\n",
    "    Args:\n",
    "        T: Sequence length\n",
    "\n",
    "    Returns:\n",
    "        train_loader    Loads training data\n",
    "        test_loader     Loads test data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    train_loader = DataLoader(AdditionDataset(train_dataset_length, len_sequence),\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers)\n",
    "    test_loader = DataLoader(AdditionDataset(test_data_length, len_sequence),\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = AdditionDataset(dataset_length=8, len_sequence=11)\n",
    "train_loader, test_loader = addition_problem(train_dataset_length=8, test_data_length=9, len_sequence=11, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.8345  0.9610  0.8239  0.4113  0.1251  0.7004  0.1098  0.6241  0.5064\n",
      "  1.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.8276  0.4372\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 0.9596\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "1\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.8337  0.2175  0.6819  0.0203  0.5571  0.0284  0.9252  0.8844  0.7368\n",
      "  1.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.0038  0.4473\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 1.3908\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "2\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.6179  0.1068  0.1715  0.4557  0.1782  0.8736  0.0532  0.2064  0.6394\n",
      "  1.0000  0.0000  0.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.4200  0.5955\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 0.7961\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "3\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.3484  0.0543  0.2626  0.7601  0.7474  0.8780  0.8138  0.9749  0.3826\n",
      "  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.1763  0.8700\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 0.6110\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "4\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.2707  0.2551  0.5133  0.9378  0.5001  0.6029  0.8910  0.7493  0.8930\n",
      "  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.0591  0.0466\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 0.7839\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "5\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.0186  0.8731  0.7955  0.8947  0.2654  0.6144  0.7886  0.2834  0.3898\n",
      "  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.1442  0.5040\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 0.8141\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "6\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.3332  0.5040  0.9572  0.8743  0.0727  0.4214  0.3373  0.1062  0.0825\n",
      "  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.4349  0.7404\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 1.2904\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "7\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.9331  0.6618  0.5782  0.0168  0.9285  0.9867  0.1088  0.8027  0.1914\n",
      "  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.2650  0.9097\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 1.5114\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n",
      "8\n",
      "{'sequence': \n",
      "(0 ,.,.) = \n",
      "\n",
      "Columns 0 to 8 \n",
      "   0.5817  0.1166  0.6618  0.4805  0.9280  0.4465  0.0577  0.1633  0.4788\n",
      "  1.0000  0.0000  1.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "\n",
      "Columns 9 to 10 \n",
      "   0.7183  0.2369\n",
      "  0.0000  0.0000\n",
      "[torch.FloatTensor of size 1x2x11]\n",
      ", 'target': \n",
      " 1.2435\n",
      "[torch.DoubleTensor of size 1]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(test_loader):\n",
    "    print(i_batch)\n",
    "    print(sample_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden))\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden = 10\n",
    "rnn = RNN(2, n_hidden, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.1326\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      "-0.3639\n",
      "-0.2442\n",
      "-0.0588\n",
      " 0.2449\n",
      "-0.0737\n",
      " 0.0707\n",
      " 0.4350\n",
      "-0.0146\n",
      " 0.0985\n",
      "-0.1212\n",
      "[torch.FloatTensor of size 10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input = Variable(sample['sequence'][:,0]) # Variable(torch.rand(2))\n",
    "hidden = Variable(torch.zeros(n_hidden))\n",
    "\n",
    "output, next_hidden = rnn(input, hidden)\n",
    "print(output)\n",
    "print(next_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "1.00000e-02 *\n",
      " -4.9688\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hidden = rnn.initHidden()\n",
    "rnn.zero_grad()\n",
    "\n",
    "sequence = Variable(sample['sequence'], requires_grad=False)\n",
    "target = Variable(torch.Tensor([sample['target']]), requires_grad=False)\n",
    "\n",
    "for i in range(t_dash):\n",
    "    input = sequence[:,i]\n",
    "    output, hidden = rnn(input, hidden)\n",
    "    \n",
    "print(output) \n",
    "print(target)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.5874\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn.train()\n",
    "for i_batch, sample_batched in enumerate(test_loader):\n",
    "    sequence = Variable(sample['sequence'], requires_grad=False)\n",
    "    target = Variable(torch.Tensor([sample['target']]), requires_grad=False)\n",
    "    \n",
    "    hidden = rnn.initHidden()\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(t_dash):\n",
    "        input = sequence[:,i]\n",
    "        output, hidden = rnn(input, hidden)\n",
    "        \n",
    "    print(output)\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(data)\n",
    "#     loss = F.nll_loss(output, target)\n",
    "#     loss.backward()\n",
    "\n",
    "#     # Clip gradients\n",
    "#     # As implemented in https://github.com/pytorch/examples/blob/master/word_language_model/main.py#L162-L164\n",
    "#     if Hyperparameters.clipping_threshold != 0:\n",
    "#         clip_grad_norm(model.parameters(), Hyperparameters.clipping_threshold)\n",
    "\n",
    "#     optimizer.step()\n",
    "#     if batch_idx % 1000 == 0:\n",
    "#         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "#             epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "#                    100.0 * batch_idx / len(train_loader), loss.data[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
