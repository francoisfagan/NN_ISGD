{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n =  2  # dimension of input\n",
    "m =  3  # dimension of output\n",
    "b =  1  # batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_logit(input, weight, bias=None):\n",
    "    \"\"\"\n",
    "    Calculate logit = input.mm(weight.t()) + bias\n",
    "\n",
    "    Args:\n",
    "        input:  [b x n]         Input vector\n",
    "        weight:  [m x n]        Weight matrix\n",
    "        bias:  [m]              Bias vector\n",
    "\n",
    "    Returns:\n",
    "        logit: [b x n]          Logit = input.mm(weight.t()) + bias\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logit = input.mm(weight.t())\n",
    "    if bias is not None:\n",
    "        logit += bias.unsqueeze(0).expand_as(logit)\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "def forward(input, weight, bias):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input [1 x n]      The input vector to the layer\n",
    "    weight [m x n]     The weight matrix of the layer\n",
    "    bias [m]           The bias vector of the layer\n",
    "    \n",
    "    Returns:\n",
    "    output [1 x m]     The input to the next layer = logit put through the non-linear activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate logit [1 x m], where logit = input.mm(weight.t()) + bias\n",
    "    logit = input.mm(weight.t())\n",
    "    if bias is not None:\n",
    "        logit += bias.unsqueeze(0).expand_as(logit)\n",
    "\n",
    "    # Non-linear activation function\n",
    "    output = torch.clamp(logit, min=0.0)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def esgd_relu(grad_output, input, weight, bias, output):\n",
    "\n",
    "    # Find all nodes where the output is greater than or equal to 0\n",
    "    ge0 = (output > 0).type(torch.FloatTensor)  # [1 x m]\n",
    "        \n",
    "    # Mask the back-propagated gradient to zero out elements where the output is zero.\n",
    "    grad_output_masked = ge0 * grad_output  # [1 x m]\n",
    "\n",
    "    # Calculate gradients\n",
    "    grad_input = grad_output_masked.mm(weight)  # [1 x n]\n",
    "    grad_weight = grad_output_masked.t().mm(input)  # [m x n]\n",
    "    grad_bias = grad_output_masked.sum(0).squeeze(0)  # [m]\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old ISGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_backwards_variables(input, weight, bias, output, logit, grad_output, lr, mu):\n",
    "    \"\"\"\n",
    "    Calculate the variables required for back-propagation\n",
    "    \n",
    "    Args:\n",
    "        saved_tensors:          Stores from forward-propagation the input, weight, bias, output\n",
    "        logit: [b x n]          Stores from forward-propagation the logit\n",
    "        grad_output: [b x m]    The gradient that has been back-propagated to this layer\n",
    "        lr: [1]                 Learning rate\n",
    "        mu: [1]                 Ridge-regularization constant\n",
    "\n",
    "    Returns:\n",
    "        input: [b x n]          Input vector\n",
    "        weight: [m x n]         Weight matrix\n",
    "        bias: [m]               Bias vector\n",
    "        output [b x m]          Input to the next layer = logit put through the non-linear activation function\n",
    "        logit: [b x n]          Logit\n",
    "        s: [b x m]              Sign of back-propagated gradient\n",
    "        z_norm: [b]             2-norm of (input, 1)\n",
    "        d: [b x m]              Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "        c: [b x m]              Logit contracted by ridge-regularization\n",
    "    \"\"\"\n",
    "\n",
    "    # ISGD constants\n",
    "    s = torch.sign(grad_output)  # [b x m]\n",
    "    z_norm = torch.sqrt(torch.norm(input, p=2, dim=1) ** 2 + 1.0)  # [b]\n",
    "    d = torch.mul(z_norm, torch.sqrt(torch.abs(grad_output)).t()).t() / math.sqrt(1.0 + lr * mu)  # [b x m]\n",
    "    c = logit / (1.0 + lr * mu)  # [b x m]\n",
    "\n",
    "    return input, weight, bias, output, logit, s, z_norm, d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the weight matrix and bias vector\n",
    "\n",
    "    Args:\n",
    "        weight: [m x n]         Weight matrix\n",
    "        mu: [1]                 Ridge-regularization constant\n",
    "        lr: [1]                 Learning rate\n",
    "        a: [b x m]              Solution of ISGD update\n",
    "        d: [b x m]              Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "        input: [b x n]          Input vector\n",
    "        z_norm: [b]             2-norm of (input, 1)\n",
    "        bias: [m]               Bias vector\n",
    "\n",
    "    Returns:\n",
    "        grad_weight: [m x n]    Gradient of the weight matrix\n",
    "        grad_bias: [m]          Gradient of the bias vector\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    grad_weight = weight * mu / (1.0 + lr * mu) - torch.mul(z_norm ** -2, (a * d).t()).mm(input)  # [m x n]\n",
    "    grad_bias = bias * mu / (1.0 + lr * mu) - torch.mul(z_norm ** -2, (a * d).t()).sum(1)  # [m]\n",
    "    return grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_relu(input, weight, bias, output, logit, grad_output):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lr = 0.01\n",
    "    mu = 0.0\n",
    "    \n",
    "    input, weight, bias, output, logit, s, z_norm, d, c = calc_backwards_variables(input, weight, bias, output, logit,\n",
    "                                                                                       grad_output, lr, mu)\n",
    "\n",
    "    # Calculate a\n",
    "    # Calculate conditions for a\n",
    "    conds0 = (s == 0).type(torch.FloatTensor)  # [b x m]\n",
    "    cond1 = ((s == +1) * (c <= 0)).type(torch.FloatTensor)  # [b x m]\n",
    "    cond2 = ((s == +1) * (c > 0) * (c <= (lr * d ** 2))).type(torch.FloatTensor)  # [b x m]\n",
    "    cond3 = ((s == +1) * (c > (lr * d ** 2))).type(torch.FloatTensor)  # [b x m]\n",
    "    cond4 = ((s == -1) * (c <= -(lr * d ** 2) / 2.0)).type(torch.FloatTensor)  # [b x m]\n",
    "    cond5 = ((s == -1) * (c > -(lr * d ** 2) / 2.0)).type(torch.FloatTensor)  # [b x m]\n",
    "\n",
    "    # Check that exactly one condition satisfied for each node\n",
    "    cond_sum = (conds0 + cond1 + cond2 + cond3 + cond4 + cond5)  # [b x m]\n",
    "    assert torch.mean(\n",
    "        (cond_sum == 1).type(torch.FloatTensor)) == 1.0, 'No implicit update condition was satisfied'\n",
    "\n",
    "    # Calculate a\n",
    "    a = (0.0 * conds0\n",
    "         + 0.0 * cond1\n",
    "         - (c / (lr * d)) * cond2\n",
    "         - d * cond3\n",
    "         + 0.0 * cond4\n",
    "         + d * cond5\n",
    "         )  # [b x m]\n",
    "\n",
    "    # a might contain Nan values if d = 0 at certain elements due to diving by d in (c / (lr * d)) * cond2\n",
    "    # The operation below sets all Nans to zero\n",
    "    # This is the appropriate value for ISGD\n",
    "    a[a != a] = 0\n",
    "\n",
    "    # Calculate input gradient\n",
    "    ge0 = (output > 0).type(torch.FloatTensor)  # [b x m]\n",
    "    grad_output_masked = ge0 * grad_output  # [b x m]\n",
    "    grad_input = grad_output_masked.mm(weight)  # [b x n]\n",
    "\n",
    "    # Calculate grad_weight, grad_bias and return all gradients\n",
    "    grad_weight, grad_bias = calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias)\n",
    "    \n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New ISGD implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_new_relu(input, weight, bias, output, logit, grad_output):\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lr = 0.00001\n",
    "    mu = 0.0\n",
    "\n",
    "    # ISGD constants\n",
    "    s = torch.sign(grad_output)  # [b x m]\n",
    "    z_norm_squared = torch.norm(input, p=2, dim=1) ** 2 + 1.0  # [b]\n",
    "    c = logit / (1.0 + lr * mu)  # [b x m]\n",
    "\n",
    "    # Calculate u\n",
    "    # Calculate conditions for u\n",
    "    threshold = lr * torch.mul(z_norm_squared, grad_output.t()).t() / (1.0 + lr * mu)  # [b x m]\n",
    "\n",
    "    cond0 = (s == 0).type(torch.FloatTensor)  # [b x m]\n",
    "    cond1 = ((s == +1) * (c <= 0)).type(torch.FloatTensor)  # [b x m]\n",
    "    cond2 = ((s == +1) * (c > 0) * (c <= threshold)).type(torch.FloatTensor)  # [b x m]\n",
    "    cond3 = ((s == +1) * (c > threshold)).type(torch.FloatTensor)  # [b x m]\n",
    "    cond4 = ((s == -1) * (c <= threshold / 2.0)).type(torch.FloatTensor)  # [b x m]\n",
    "    cond5 = ((s == -1) * (c > threshold / 2.0)).type(torch.FloatTensor)  # [b x m]\n",
    "\n",
    "    # Check that exactly one condition satisfied for each node\n",
    "    cond_sum = (cond0 + cond1 + cond2 + cond3 + cond4 + cond5)  # [b x m]\n",
    "    assert torch.mean(\n",
    "        (cond_sum == 1).type(torch.FloatTensor)) == 1.0, 'No implicit update condition was satisfied'\n",
    "\n",
    "    # Calculate u\n",
    "    u = (0.0 * (cond0 + cond1 + cond4)\n",
    "         + torch.div(c.t(), z_norm_squared).t() / lr * cond2\n",
    "         + grad_output / (1.0 + lr * mu) * (cond3 + cond5)\n",
    "         )  # [b x m]\n",
    "\n",
    "    # a might contain Nan values if d = 0 at certain elements due to diving by d in (c / (lr * d)) * cond2\n",
    "    # The operation below sets all Nans to zero\n",
    "    # This is the appropriate value for ISGD\n",
    "    u[u != u] = 0\n",
    "\n",
    "    # Calculate input gradient\n",
    "    ge0 = (output > 0).type(torch.FloatTensor)  # [b x m]\n",
    "    grad_output_masked = ge0 * grad_output  # [b x m]\n",
    "    grad_input = grad_output_masked.mm(weight)  # [b x n]\n",
    "\n",
    "    # Calculate grad_weight, grad_bias\n",
    "    grad_weight = weight * mu / (1.0 + lr * mu) + u.t().mm(input)  # [m x n]\n",
    "    grad_bias = bias * mu / (1.0 + lr * mu) + u.t().sum(1)  # [m]\n",
    "    \n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the differences between ESGD, ISGD old and ISGD_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Difference between ESGD and ISGD new\n",
      "[\n",
      " 0  0\n",
      "[torch.FloatTensor of size 1x2]\n",
      ", \n",
      " 0  0\n",
      " 0  0\n",
      " 0  0\n",
      "[torch.FloatTensor of size 3x2]\n",
      ", \n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 3]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Random data\n",
    "grad_output = torch.randn(b, m)     # [b x m]\n",
    "input = torch.randn(b, n)           # [b x n]\n",
    "weight = torch.randn(m, n)          # [m x n]\n",
    "bias = torch.randn(m,)              # [m]\n",
    "\n",
    "# Check that forward propagation makes sense\n",
    "logit = calc_logit(input, weight, bias)\n",
    "output = forward(input, weight, bias)\n",
    "\n",
    "# Calculate gradients\n",
    "esgd_grads = esgd_relu(grad_output, input, weight, bias, output)\n",
    "isgd_new_grads = isgd_new_relu(input, weight, bias, output, logit, grad_output)\n",
    "isgd_grads = isgd_relu(input, weight, bias, output, logit, grad_output)\n",
    "\n",
    "\n",
    "# Print difference\n",
    "# print('Difference between ESGD and ISGD old')\n",
    "# print([(x-y) for x,y in zip(isgd_grads, esgd_grads)])\n",
    "\n",
    "print('\\nDifference between ESGD and ISGD new')\n",
    "print([(x-y) for x,y in zip(isgd_new_grads, esgd_grads)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
