{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "mu = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_output:  \n",
      "-0.1607 -0.2673  0.0717  0.9223 -2.7848\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grad_output = torch.randn(1, 5)\n",
    "print('grad_output: ', grad_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  \n",
      " 0.5483 -1.5596  1.0649\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "weight:  torch.Size([5, 3])\n",
      "bias:  torch.Size([5])\n",
      "output:  torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Forward propagation\n",
    "input = torch.randn(1, 3)\n",
    "weight = torch.randn(5, 3)\n",
    "bias = torch.randn(5,)\n",
    "output = input.mm(weight.t())\n",
    "output += bias.unsqueeze(0).expand_as(output)\n",
    "\n",
    "print('input: ', input)\n",
    "print('weight: ', weight.size())\n",
    "print('bias: ', bias.size())\n",
    "print('output: ', output.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.7658  1.6261 -5.1807\n",
      "[torch.FloatTensor of size 1x3]\n",
      " \n",
      "-0.0881  0.2506 -0.1711\n",
      "-0.1466  0.4169 -0.2846\n",
      " 0.0393 -0.1119  0.0764\n",
      " 0.5057 -1.4384  0.9821\n",
      "-1.5268  4.3431 -2.9655\n",
      "[torch.FloatTensor of size 5x3]\n",
      " \n",
      "-0.1607\n",
      "-0.2673\n",
      " 0.0717\n",
      " 0.9223\n",
      "-2.7848\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grad_input = grad_weight = grad_bias = None\n",
    "grad_input = grad_output.mm(weight)\n",
    "grad_weight = grad_output.t().mm(input)\n",
    "grad_bias = grad_output.sum(0).squeeze(0)\n",
    "\n",
    "print(grad_input, grad_weight, grad_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha_linear(s,d,c):\n",
    "    return -s.mul(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.7658  1.6261 -5.1807\n",
      "[torch.FloatTensor of size 1x3]\n",
      " \n",
      "-0.0881  0.2506 -0.1711\n",
      "-0.1466  0.4169 -0.2846\n",
      " 0.0393 -0.1119  0.0764\n",
      " 0.5057 -1.4384  0.9821\n",
      "-1.5268  4.3431 -2.9655\n",
      "[torch.FloatTensor of size 5x3]\n",
      " \n",
      "-0.1607\n",
      "-0.2673\n",
      " 0.0717\n",
      " 0.9223\n",
      "-2.7848\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "s = torch.sign(grad_output)\n",
    "z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0))\n",
    "d = z_norm * math.sqrt(lr/(1.0+lr*mu)) * torch.sqrt(torch.abs(grad_output))\n",
    "alpha = alpha_linear(s,d,c)\n",
    "\n",
    "new_weight = weight / (1.0 + lr * mu) + (alpha.mul(d)).t().mm(input) / z_norm **2\n",
    "grad_weight = (weight - new_weight) / lr\n",
    "\n",
    "new_bias = bias / (1.0 + lr * mu) + alpha.mul(d).squeeze() / z_norm **2\n",
    "grad_bias = (bias - new_bias) / lr\n",
    "\n",
    "print(grad_input, grad_weight, grad_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-1.4060 -0.7285 -0.0530\n",
      " 0.1984 -1.0676 -1.0577\n",
      " 0.0716  0.4500  0.0120\n",
      "-0.7115 -0.0378 -1.5769\n",
      "-0.8058 -0.4403  1.4430\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.0078  0.0130 -0.0035 -0.0449  0.1355\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n",
      "\n",
      "-0.0110 -0.0057 -0.0004\n",
      " 0.0026 -0.0139 -0.0138\n",
      "-0.0002 -0.0016 -0.0000\n",
      " 0.0319  0.0017  0.0708\n",
      "-0.1092 -0.0597  0.1956\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected r_ [5 x 3], t [5 x 3] and src [1 x 5] to have the same number of elements, but got 15, 15 and 5 elements respectively at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensorMath.c:1036",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-c3ec096e86e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.4060\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0078\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.7285\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0078\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.0530\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.0078\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0m__rmul__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected r_ [5 x 3], t [5 x 3] and src [1 x 5] to have the same number of elements, but got 15, 15 and 5 elements respectively at /Users/soumith/minicondabuild3/conda-bld/pytorch_1518385717421/work/torch/lib/TH/generic/THTensorMath.c:1036"
     ]
    }
   ],
   "source": [
    "v = alpha.mul(d)\n",
    "print(weight)\n",
    "print(v)\n",
    "print(v.t() * weight)\n",
    "print(weight * v.t())\n",
    "print([-1.4060*0.0078, -0.7285*0.0078, -0.0530*0.0078])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.8381\n",
      " 0.2784\n",
      " 0.0492\n",
      "-0.8235\n",
      " 0.2812\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      " 0.8381\n",
      " 0.2784\n",
      " 0.0492\n",
      "-0.8235\n",
      " 0.2812\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "\n",
      " 2.4169  1.6287 -3.7273\n",
      "[torch.FloatTensor of size 1x3]\n",
      " \n",
      " 1.7658  1.6261 -5.1807\n",
      " 1.7659  1.6261 -5.1807\n",
      " 1.7658  1.6261 -5.1807\n",
      " 1.7658  1.6261 -5.1807\n",
      " 1.7658  1.6261 -5.1807\n",
      "[torch.FloatTensor of size 5x3]\n",
      " \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Note that torch.norm outputs a float instead of a tensor\n",
    "z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0))\n",
    "d = z_norm * math.sqrt(lr/(1.0+lr*mu)) * torch.sqrt(abs_grad_output)\n",
    "c = output/(1.0+lr*mu)\n",
    "# print('s: ', s)\n",
    "# print('delta: ', d) \n",
    "# print(c)\n",
    "\n",
    "# Calculate alpha\n",
    "alpha = alpha_linear(s,d,c)\n",
    "# print(alpha)\n",
    "\n",
    "# Calculate gradients\n",
    "new_weight = weight / (1.0 + lr * mu) + alpha.mul(d).mm(weight) / z_norm**2\n",
    "grad_weight = (weight - new_weight) / lr\n",
    "# print(weight)\n",
    "# print(new_weight)\n",
    "# print(grad_weight)\n",
    "\n",
    "new_bias = bias / (1.0 + lr * mu) #+ alpha.mul(d).squeeze().mul(bias) / z_norm**2\n",
    "grad_bias = (bias - new_bias) / lr\n",
    "# print(bias)\n",
    "# print(new_bias)\n",
    "# print(grad_bias)\n",
    "\n",
    "\n",
    "sgn_output = (output >= 0).type(torch.FloatTensor)\n",
    "grad_input = (grad_output.mul(sgn_output)).mm(weight)\n",
    "# print(grad_input)\n",
    "\n",
    "print(grad_input, grad_weight, grad_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3)\n",
    "weight = torch.randn(5, 3)\n",
    "bias = torch.randn(5,)\n",
    "output = input.mm(weight.t())\n",
    "output += bias.unsqueeze(0).expand_as(output)\n",
    "relu = torch.clamp(output, min=0.0)\n",
    "\n",
    "print('input: ', input)\n",
    "print('weight: ', weight.size())\n",
    "print('bias: ', bias.size())\n",
    "print('output: ', output.size())\n",
    "print('relu: ', relu.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "s = torch.sign(grad_output)\n",
    "abs_grad_output = torch.abs(grad_output)\n",
    "# Note that torch.norm outputs a float instead of a tensor\n",
    "z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0))\n",
    "d = z_norm * math.sqrt(lr/(1.0+lr*mu)) * torch.sqrt(abs_grad_output)\n",
    "c = output/(1.0+lr*mu)\n",
    "# print('s: ', s)\n",
    "# print('delta: ', d) \n",
    "# print(c)\n",
    "\n",
    "# Calculate alpha\n",
    "alpha = alpha_relu(s,d,c)\n",
    "\n",
    "# Calculate gradients\n",
    "new_weight = weight / (1.0 + lr * mu) + alpha.mul(d).mm(weight) / z_norm**2\n",
    "grad_weight = (weight - new_weight) / lr\n",
    "# print(weight)\n",
    "# print(new_weight)\n",
    "# print(grad_weight)\n",
    "\n",
    "new_bias = bias / (1.0 + lr * mu) + alpha.mul(d).squeeze().mul(bias) / z_norm**2\n",
    "grad_bias = (bias - new_bias) / lr\n",
    "# print(bias)\n",
    "# print(new_bias)\n",
    "# print(grad_bias)\n",
    "\n",
    "sgn_output = (output >= 0).type(torch.FloatTensor)\n",
    "grad_input = (grad_output.mul(sgn_output)).mm(weight)\n",
    "print(grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha_relu(s,d,c):\n",
    "#     cond1 = (s == 1).mul(c <= 0).type(torch.FloatTensor)\n",
    "    cond2 = (s == 1).mul(c > 0).mul(c <= d**2).type(torch.FloatTensor)\n",
    "    cond3 = (s == 1).mul(c > d**2).type(torch.FloatTensor)\n",
    "#     cond4 = (s == -1).mul(c <= -d**2/2.0).type(torch.FloatTensor)\n",
    "    cond5 = (s == -1).mul(c > -d**2/2.0).type(torch.FloatTensor)\n",
    "    # print(cond1, cond2, cond3, cond4, cond5)\n",
    "\n",
    "    alpha = (0.0\n",
    "#              + 0.0 * cond1\n",
    "            - (c.div(d)).mul(cond2)\n",
    "            - d.mul(cond3)\n",
    "#             + 0.0 * cond4\n",
    "            + d.mul(cond5)\n",
    "            )\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_relu(s,d,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand grad_output_pos_out.sum(0).squeeze(0)\n",
    "print(grad_output_pos_out)\n",
    "print(grad_output_pos_out.sum(0))\n",
    "print(grad_output_pos_out.sum(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_out = (output >= 0).type(torch.FloatTensor)\n",
    "grad_output_pos_out = torch.mul(grad_output, pos_out)\n",
    "grad_input = grad_output_pos_out.mm(weight)\n",
    "grad_weight = grad_output_pos_out.t().mm(input)\n",
    "grad_bias = grad_output_pos_out.sum(0).squeeze(0)\n",
    "\n",
    "print('grad_output: ', grad_output.size())\n",
    "print('output: ', output.size())\n",
    "print('pos_out: ', pos_out.size())\n",
    "print('grad_output_pos_out: ', grad_output_pos_out.size())\n",
    "print('grad_input: ', grad_input.size())\n",
    "print('grad_bias: ', grad_bias.size())\n",
    "print('grad_weight: ', grad_weight.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
