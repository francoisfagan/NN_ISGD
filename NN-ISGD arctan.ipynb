{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn = dimension of input\\nm = dimension of output\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "n = dimension of input\n",
    "m = dimension of output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_logit(input, weight, bias=None):\n",
    "    \"\"\"\n",
    "    Calculate logit = input.mm(weight.t()) + bias\n",
    "\n",
    "    Args:\n",
    "        input:  [1 x n]         Input vector\n",
    "        weight:  [m x n]        Weight matrix\n",
    "        bias:  [m]              Bias vector\n",
    "\n",
    "    Returns:\n",
    "        logit: [1 x n]          Logit = input.mm(weight.t()) + bias\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    logit = input.mm(weight.t())\n",
    "    if bias is not None:\n",
    "        logit += bias.unsqueeze(0).expand_as(logit)\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def esgd_arctan(grad_output, input, weight, bias, output):\n",
    "\n",
    "    grad_output_scaled = grad_output / (1 + logit ** 2)  # [1 x m]\n",
    "    grad_input = grad_output_scaled.mm(weight)  # [1 x n]\n",
    "    grad_weight = grad_output_scaled.t().mm(input)  # [m x n]\n",
    "    grad_bias = grad_output_scaled.sum(0).squeeze(0)  # [m]\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  0.3953  0.2813\n",
       " [torch.FloatTensor of size 1x2], \n",
       "  0.1423 -0.0742\n",
       " -0.6416  0.3346\n",
       "  0.0886 -0.0462\n",
       " [torch.FloatTensor of size 3x2], \n",
       "  0.1608\n",
       " -0.7249\n",
       "  0.1001\n",
       " [torch.FloatTensor of size 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "esgd_arctan(grad_output, input, weight, bias, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_backwards_variables(saved_tensors, logit, grad_output, lr, mu):\n",
    "    \"\"\"\n",
    "    Calculate the variables required for back-propagation\n",
    "    \n",
    "    Args:\n",
    "        saved_tensors:          Stores from forward-propagation the input, weight, bias, output\n",
    "        logit: [1 x n]          Stores from forward-propagation the logit\n",
    "        grad_output: [1 x m]    The gradient that has been back-propagated to this layer\n",
    "        lr: [1]                 Learning rate\n",
    "        mu: [1]                 Ridge-regularization constant\n",
    "\n",
    "    Returns:\n",
    "        input: [1 x n]          Input vector\n",
    "        weight: [m x n]         Weight matrix\n",
    "        bias: [m]               Bias vector\n",
    "        output [1 x m]          Input to the next layer = logit put through the non-linear activation function\n",
    "        logit: [1 x n]          Logit\n",
    "        s: [1 x m]              Sign of back-propagated gradient\n",
    "        z_norm: [1]             2-norm of (input, 1)\n",
    "        d: [1 x m]              Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "        c: [1 x m]              Logit contracted by ridge-regularization\n",
    "    \"\"\"\n",
    "\n",
    "    # Unpack saved values\n",
    "    input, weight, bias, output = saved_tensors\n",
    "\n",
    "    # ISGD constants\n",
    "    s = torch.sign(grad_output)\n",
    "    z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0))\n",
    "    d = z_norm / math.sqrt(1.0 + lr * mu) * torch.sqrt(torch.abs(grad_output))\n",
    "    c = logit / (1.0 + lr * mu)\n",
    "\n",
    "    return input, weight, bias, output, logit, s, z_norm, d, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the weight matrix and bias vector\n",
    "\n",
    "    Args:\n",
    "        weight: [m x n]         Weight matrix\n",
    "        mu: [1]                 Ridge-regularization constant\n",
    "        lr: [1]                 Learning rate\n",
    "        a: [1 x m]              Solution of ISGD update\n",
    "        d: [1 x m]              Weighted constant, proportional to the sqrt(abs(back-propagated gradient))\n",
    "        input: [1 x n]          Input vector\n",
    "        z_norm: [1]             2-norm of (input, 1)\n",
    "        bias: [m]               Bias vector\n",
    "\n",
    "    Returns:\n",
    "        grad_weight: [m x n]    Gradient of the weight matrix\n",
    "        grad_bias: [m]          Gradient of the bias vector\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    grad_weight = weight * mu / (1.0 + lr * mu) - (a * d).t().mm(input) / z_norm ** 2\n",
    "    grad_bias = bias * mu / (1.0 + lr * mu) - (a * d).squeeze() / z_norm ** 2\n",
    "    return grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_root_closest_to_zero(coeff):\n",
    "    roots = np.roots(coeff)\n",
    "    real_roots = [root.real for root in roots if root.imag == 0]\n",
    "    root_closest_to_zero = reduce((lambda x, y: x if (abs(x) < abs(y)) else y), real_roots)\n",
    "    return root_closest_to_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_arctan(grad_output, input, weight, bias, output, logit):\n",
    "    \n",
    "    lr, mu = 0.00000001, 0.0\n",
    "    input, weight, bias, output, logit, s, z_norm, d, c = calc_backwards_variables([input, weight, bias, output], logit,\n",
    "                                                                                       grad_output, lr, mu)\n",
    "\n",
    "    # Calculate a\n",
    "        coeff = np.array([((lr * d) ** 2).numpy()[0],\n",
    "                          (2 * lr * d * c).numpy()[0],\n",
    "                          (c ** 2 + 1).numpy()[0],\n",
    "                          (s * d).numpy()[0]])\n",
    "\n",
    "        root_closest_to_zero = np.apply_along_axis(real_root_closest_to_zero, 0, coeff)\n",
    "        a = torch.from_numpy(root_closest_to_zero).unsqueeze(1).t().type(torch.FloatTensor)\n",
    "\n",
    "        # Calculate grad_weight, grad_bias and return all gradients\n",
    "        grad_weight, grad_bias = calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias)\n",
    "\n",
    "        # Calculate input gradient\n",
    "        grad_output_scaled = grad_output / (1 + logit ** 2)  # [1 x m]\n",
    "        grad_input = grad_output_scaled.mm(weight)  # [1 x n]\n",
    "\n",
    "        return grad_input, grad_weight, grad_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isgd_arctan2(grad_output, input, weight, bias, output, logit):\n",
    "    \n",
    "    lr, mu = 0.00000001, 0.0\n",
    "    input, weight, bias, output, logit, s, z_norm, d, c = calc_backwards_variables([input, weight, bias, output], logit,\n",
    "                                                                                       grad_output, lr, mu)\n",
    "\n",
    "    # Calculate a\n",
    "    a = d*0\n",
    "    diff = 1\n",
    "    iter_count = 0\n",
    "    while diff < 1e-10:\n",
    "        a_new = - s * d / (1.0 + (lr * d * a + c)**2)\n",
    "        diff = torch.norm(a - a_new)\n",
    "        a = a_new\n",
    "        iter_count += 1\n",
    "        assert(iter_count < 20), 'Arctan update has failed to converge'\n",
    "\n",
    "    # Calculate grad_weight, grad_bias and return all gradients\n",
    "    grad_weight, grad_bias = calc_weigh_bias_grad(weight, mu, lr, a, d, input, z_norm, bias)\n",
    "\n",
    "    # Calculate input gradient\n",
    "    grad_output_scaled = grad_output / (1 + logit ** 2)  # [1 x m]\n",
    "    grad_input = grad_output_scaled.mm(weight)  # [1 x n]\n",
    "\n",
    "    return grad_input, grad_weight, grad_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1638623909718155\n",
      "0.010818114720922343\n",
      "0.00010266294157844973\n",
      "9.5367431640625e-07\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "bot",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-185-0f58b74d15d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0miter_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_count\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bot'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: bot"
     ]
    }
   ],
   "source": [
    "# Calculate a\n",
    "a = d*0\n",
    "diff = 1\n",
    "iter_count = 0\n",
    "while diff < 1e10:\n",
    "    a_new = - s * d / (1.0 + (lr * d * a + c)**2)\n",
    "    diff = torch.norm(a - a_new)\n",
    "    print(diff)\n",
    "    a = a_new\n",
    "    iter_count += 1\n",
    "    assert(iter_count < 20), 'Arctan update has failed to converge'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the differences between ESGD and ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 0  0\n",
      "[torch.FloatTensor of size 1x2]\n",
      ", \n",
      "1.00000e-07 *\n",
      "  0.0093  0.0000\n",
      " -0.2980 -0.5960\n",
      "  0.8941  1.7881\n",
      "[torch.FloatTensor of size 3x2]\n",
      ", \n",
      "1.00000e-07 *\n",
      "  0.0000\n",
      "  0.5960\n",
      " -2.3842\n",
      "[torch.FloatTensor of size 3]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Random data\n",
    "grad_output = torch.randn(1, 3)     # [1 x m]\n",
    "input = torch.randn(1, 2)           # [1 x n]\n",
    "weight = torch.randn(3, 2)          # [m x n]\n",
    "bias = torch.randn(3,)              # [m]\n",
    "\n",
    "# Forward propagation\n",
    "# Calculate logit [1 x m], where logit = input.mm(weight.t()) + bias\n",
    "logit = calc_logit(input, weight, bias)\n",
    "\n",
    "# Non-linear activation function\n",
    "output = torch.atan(logit)  # [1 x m]\n",
    "\n",
    "# print('logit: ', logit)\n",
    "# print('output: ', output)\n",
    "\n",
    "# isgd_grads = isgd_arctan(grad_output, input, weight, bias, output, logit)\n",
    "isgd_grads2 = isgd_arctan2(grad_output, input, weight, bias, output, logit)\n",
    "esgd_grads = esgd_arctan(grad_output, input, weight, bias, output)\n",
    "\n",
    "# print(isgd_grads)\n",
    "# print(esgd_grads)\n",
    "print([(x-y) for x,y in zip(isgd_grads2, esgd_grads)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Pytorch module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3)\n",
    "weight = torch.randn(5, 3)\n",
    "bias = torch.randn(5,)\n",
    "output = input.mm(weight.t())\n",
    "output += bias.unsqueeze(0).expand_as(output)\n",
    "relu = torch.clamp(output, min=0.0)\n",
    "\n",
    "print('input: ', input)\n",
    "print('weight: ', weight.size())\n",
    "print('bias: ', bias.size())\n",
    "print('output: ', output.size())\n",
    "print('relu: ', relu.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "s = torch.sign(grad_output)\n",
    "abs_grad_output = torch.abs(grad_output)\n",
    "# Note that torch.norm outputs a float instead of a tensor\n",
    "z_norm = math.sqrt((torch.norm(input) ** 2 + 1.0))\n",
    "d = z_norm * math.sqrt(lr/(1.0+lr*mu)) * torch.sqrt(abs_grad_output)\n",
    "c = output/(1.0+lr*mu)\n",
    "# print('s: ', s)\n",
    "# print('delta: ', d) \n",
    "# print(c)\n",
    "\n",
    "# Calculate alpha\n",
    "alpha = alpha_relu(s,d,c)\n",
    "\n",
    "# Calculate gradients\n",
    "new_weight = weight / (1.0 + lr * mu) + alpha.mul(d).mm(weight) / z_norm**2\n",
    "grad_weight = (weight - new_weight) / lr\n",
    "# print(weight)\n",
    "# print(new_weight)\n",
    "# print(grad_weight)\n",
    "\n",
    "new_bias = bias / (1.0 + lr * mu) + alpha.mul(d).squeeze().mul(bias) / z_norm**2\n",
    "grad_bias = (bias - new_bias) / lr\n",
    "# print(bias)\n",
    "# print(new_bias)\n",
    "# print(grad_bias)\n",
    "\n",
    "sgn_output = (output >= 0).type(torch.FloatTensor)\n",
    "grad_input = (grad_output.mul(sgn_output)).mm(weight)\n",
    "print(grad_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alpha_relu(s,d,c):\n",
    "#     cond1 = (s == 1).mul(c <= 0).type(torch.FloatTensor)\n",
    "    cond2 = (s == 1).mul(c > 0).mul(c <= d**2).type(torch.FloatTensor)\n",
    "    cond3 = (s == 1).mul(c > d**2).type(torch.FloatTensor)\n",
    "#     cond4 = (s == -1).mul(c <= -d**2/2.0).type(torch.FloatTensor)\n",
    "    cond5 = (s == -1).mul(c > -d**2/2.0).type(torch.FloatTensor)\n",
    "    # print(cond1, cond2, cond3, cond4, cond5)\n",
    "\n",
    "    alpha = (0.0\n",
    "#              + 0.0 * cond1\n",
    "            - (c.div(d)).mul(cond2)\n",
    "            - d.mul(cond3)\n",
    "#             + 0.0 * cond4\n",
    "            + d.mul(cond5)\n",
    "            )\n",
    "\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha_relu(s,d,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Understand grad_output_pos_out.sum(0).squeeze(0)\n",
    "print(grad_output_pos_out)\n",
    "print(grad_output_pos_out.sum(0))\n",
    "print(grad_output_pos_out.sum(0).squeeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_out = (output >= 0).type(torch.FloatTensor)\n",
    "grad_output_pos_out = torch.mul(grad_output, pos_out)\n",
    "grad_input = grad_output_pos_out.mm(weight)\n",
    "grad_weight = grad_output_pos_out.t().mm(input)\n",
    "grad_bias = grad_output_pos_out.sum(0).squeeze(0)\n",
    "\n",
    "print('grad_output: ', grad_output.size())\n",
    "print('output: ', output.size())\n",
    "print('pos_out: ', pos_out.size())\n",
    "print('grad_output_pos_out: ', grad_output_pos_out.size())\n",
    "print('grad_input: ', grad_input.size())\n",
    "print('grad_bias: ', grad_bias.size())\n",
    "print('grad_weight: ', grad_weight.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
